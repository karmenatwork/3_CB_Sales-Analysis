{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sales "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import utils\n",
    "\n",
    "print(\"All packages imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe from csv file\n",
    "df = pd.read_csv(\"AusApparalSales4thQrt2020.csv\")\n",
    "print(f\"df.head()\\n {df.head()} \\n\")\n",
    "print(f\"df.tail() \\n {df.tail()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.describe()\n",
    "print(f\"size: {df.size}\\n\")\n",
    "print(f\"memory_usage: \\n{df.memory_usage(deep=True)} \\n\")\n",
    "print(f\"{df.dtypes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize memory\n",
    "# Convert object type to str, Date to datetime and reduce int.\n",
    "cols = df.select_dtypes(np.object_).columns[1:]  # every object type except Date\n",
    "df[cols] = df[cols].astype(\"string\")\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "df[\"Unit\"] = df[\"Unit\"].astype(\"int8\")\n",
    "df[\"Sales\"] = df[\"Sales\"].astype(\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.set_index(['Date'], inplace=True)\n",
    "# df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"After optimizing\")\n",
    "print(f\"size: {df.size}\\n\")\n",
    "print(f\"memory_usage: \\n{df.memory_usage(deep=True)}\\n\")\n",
    "print(f\"{df.dtypes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "\n",
    "We can see that at least in `Date`, 'Unit`and 'Sales` the memory_usage reduced. It wasn't the case for Time, State and Group. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing Time, State, Group back to object\n",
    "df[cols] = df[cols].astype(object)\n",
    "print(\"After changing back\")\n",
    "print(f\"memory_usage: \\n{df.memory_usage(deep=True)}\\n\")\n",
    "print(f\"{df.dtypes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions = df.shape\n",
    "print(\n",
    "    f\"Dimensions {dimensions} Data contains {dimensions[0]} rows and {dimensions[1]} columns\"\n",
    ")\n",
    "print(f\"Columns are {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data wrangling\n",
    "\n",
    "a. Ensure that the data is clean and free from any missing or incorrect entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find rows with at least one NA value\n",
    "df[df.isna().any(axis=1)]\n",
    "\n",
    "# Find rows where all values are empty\n",
    "# df[df.isna().all(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find columns with at least one empty value\n",
    "df.columns[df.isna().any()]\n",
    "\n",
    "# Find columns where all values are empty\n",
    "# df.columns[df.isna().all()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for null in any of the columns\n",
    "# df.isna().sum()\n",
    "df.notna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "\n",
    "In the result, we can see that the data does not have null values:\n",
    "\n",
    "- `df.isna().sum()`returns 0 for each column\n",
    "- `df.notnat().sum()` returns 7560 for each columns, which tells me that there are not NaN\n",
    "\n",
    "However, `isna()` or `df.notna()` doesn't seen to check for missing values (spaces, empty string)\n",
    "\n",
    "So I added NaN and empty string to the dataframe to make sure I check for empty string as well as NaN values.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import locale\n",
    "\n",
    "# Note:\n",
    "# I was getting \"time data '29-Dec-2020' does not match format '%d-%b-%Y'\"\"\n",
    "# The %b directive in the strptime method is locale-dependent so I set it to EN-US\n",
    "\n",
    "# Set the locale to English\n",
    "locale.setlocale(locale.LC_ALL, \"en_US.UTF-8\")\n",
    "\n",
    "# 20-Dec-2020,'Afternoon', '', Seniors, 13, 32500\n",
    "# 30-Dec-2020','Evening', 'TAS',None,12, 33500\n",
    "# 31-Dec-2020',np.nan, 'TAS','Kids',12, 3350\n",
    "\n",
    "# Add 3 rows with empty values, None and NaN to the dataframe\n",
    "df2 = df.copy()\n",
    "df2.loc[len(df2.index)] = [\n",
    "    datetime.strptime(\"29-Dec-2020\", \"%d-%b-%Y\"),\n",
    "    \"Afternoon\",\n",
    "    \"\",\n",
    "    \"Seniors\",\n",
    "    12,\n",
    "    32500,\n",
    "]  # empty value in State\n",
    "df2.loc[len(df2.index)] = [\n",
    "    datetime.strptime(\"30-Dec-2020\", \"%d-%b-%Y\"),\n",
    "    \"Evening\",\n",
    "    \"TAS\",\n",
    "    None,\n",
    "    12,\n",
    "    33500,\n",
    "]  # None value in Group\n",
    "df2.loc[len(df2.index)] = [\n",
    "    datetime.strptime(\"31-Dec-2020\", \"%d-%b-%Y\"),\n",
    "    np.nan,\n",
    "    \"TAS\",\n",
    "    \"Kids\",\n",
    "    12,\n",
    "    33500,\n",
    "]  # Nan value in Time\n",
    "df2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Before optimization\")\n",
    "print(f\"df2.dtypes: \\n{df2.dtypes}\\n\")\n",
    "print(f\"memory_usage: \\n{df2.memory_usage(deep=True)}\\n\")\n",
    "\n",
    "# Optimize df2 too \n",
    "df2[\"Unit\"] = df2[\"Unit\"].astype(\"int8\")\n",
    "df2[\"Sales\"] = df2[\"Sales\"].astype(\"int32\")\n",
    "\n",
    "print(f\"After optimization\")\n",
    "print(f\"df2.dtypes: \\n{df2.dtypes}\\n\")\n",
    "print(f\"memory_usage: \\n{df2.memory_usage(deep=True)}\\n\")\n",
    "\n",
    "# df2.select_dtypes(np.object_).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for null with numpy\n",
    "print(\n",
    "    f\"Using numPy isnull(), it only detects None and NaN \\nSo running 'np.where(pd.isnull(df2)' returns: {np.where(pd.isnull(df2))}\"\n",
    ")\n",
    "print(\n",
    "    \"Indicating that in row 7561 column 3 there is a None value,  and in row 7562 column 3 there is a NaN value  \\n\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Same as isna(), it only detects None and NaN\\n So running, 'df2[df2.isna().any(axis=1)])': \\n{df2[df2.isna().any(axis=1)]} \\n\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"In conclusion, we need to check for '' (empty) values as well df2[df2.map(lambda x: x == '').any(axis=1)]\\n\"\n",
    ")\n",
    "print(f\"{df2[df2.map(lambda x: x == '').any(axis=1)]} \\n\")\n",
    "print(f\"And we combine checking for NaN and '' empty values to \")\n",
    "\n",
    "# Using isna()\n",
    "# df2[df2.isna().any(axis=1) | df2.map(lambda x: x == '').any(axis=1)]\n",
    "\n",
    "# Using isnull\n",
    "df2[df2.isnull().any(axis=1) | (df2 == \"\").any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check for the columns\n",
    "# cols_with_empty = df2.apply(lambda col: col.apply(lambda x: x.strip() == '' if isinstance(x, str) else False).any())\n",
    "# print(cols_with_empty[cols_with_empty].index.tolist())\n",
    "df2.columns[df2.isnull().any() | (df2 == \"\").any()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "\n",
    "Using numpy we can see:\n",
    "\n",
    "- `np.where(pd.isnull(df))` returns the row and column indices where the value is NaN. We get empty arrays as a result\n",
    "- `np.where(df.map(lambda x: x == ''))` also returns empty arrays. \n",
    "\n",
    "Note that using `map` requires calling a Python function once for each cell of the DataFrame. That could be slow for a large DataFrame, so it would be better \n",
    "to arrange for all the blank cells to contain NaN instead, and then use `pd.isnull()`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Based on your knowledge of Data Analytics, include your recommendations for treating missing and incorrect data (dropping the null values or filling them)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recomendations\n",
    "\n",
    "There are a couple things we could do. \n",
    "- Per above comment, for all the blank cells I will fill with NaN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with NaN and then find those rows.\n",
    "df = df.replace(\" \", np.nan)\n",
    "nan_values = df[df.isna().any(axis=1)]\n",
    "nan_values\n",
    "\n",
    "nan_rows = df.loc[df.isna().any(axis=1)]\n",
    "nan_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will use df2 to make my recommendation in the case of missing/incorrect data\n",
    "\n",
    "# 1. First replace '' with NaN\n",
    "\n",
    "# df2.replace('None', np.nan, inplace=True) and find them\n",
    "df2 = df2.replace(\"\", np.nan)\n",
    "\n",
    "# Find Options\n",
    "# a. Using `df.loc``\n",
    "# nan_rows  = df2.loc[df2.isna().any(axis=1)]\n",
    "\n",
    "# b. finding all `isnull()` rows first\n",
    "# null_mask = df2.isnull().any(axis=1)\n",
    "# null_rows = df2[null_mask]\n",
    "# null_rows\n",
    "\n",
    "# c. or directly finding all `isna()`` rows\n",
    "nan_rows = df2[df2.isna().any(axis=1)]\n",
    "nan_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Then, make sure that each value is unique, so we can groupby later.\n",
    "categorical = [\"Time\", \"State\", \"Group\"]\n",
    "df2_categorical = df2[categorical]\n",
    "\n",
    "# a. describe() will give you the unique info\n",
    "df2_categorical.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_categorical.info()\n",
    "df2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b. Or inspecting with `nunique()` and `unique()`\n",
    "if df2.Time.nunique() != len(df2.Time.unique()):\n",
    "    print(f\"Time values {df2.Time.unique()} - {df2.Time.nunique()}\")\n",
    "\n",
    "if df2.State.nunique() != len(df2.State.unique()):\n",
    "    print(f\"State values {df2.State.unique()} - {df2.State.nunique()}\")\n",
    "\n",
    "if df2.Group.nunique() != len(df2.Group.unique()):\n",
    "    print(f\"Group values {df2.Group.unique()} - {df2.Group.nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "We can see that some categorical values has spaces. For example, these are two different values.\n",
    "- For Time, ' Evening' and 'Evening'  \n",
    "- For State, ' TAS' isn't the same as 'TAS'\n",
    "- For Group, ' Kids'and 'Kids'. ' Seniors' vs 'Seniors'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c. with value_counts() we can also see that something isn't quite right. (What are the possible values that belogn to this category)\n",
    "for colName in list(df2_categorical):\n",
    "    print(f\"{colName} counts: \\n {df2[colName].value_counts()} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = df2.loc[7560: 7564]['Time']\n",
    "states = df2.loc[7560: 7564]['State']\n",
    "group = df2.loc[7560: 7564]['Group']\n",
    "\n",
    "print(times)\n",
    "print(states)\n",
    "print(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Remove spaces to then groupby unique values\n",
    "# ====================\n",
    "# Not sure why this isn't working\n",
    "# for colName in list(df2_categorical):\n",
    "#     df[colName] = df[colName].apply(lambda x: x.strip())\n",
    "#     print(f\"{colName} values: {df2[colName].unique()} - {df2[colName].nunique()}\")\n",
    "\n",
    "df2[\"Time\"] = df2[\"Time\"].str.strip()\n",
    "df2[\"State\"] = df2[\"State\"].str.strip()\n",
    "df2[\"Group\"] = df2[\"Group\"].str.strip()\n",
    "\n",
    "print(f\"Time values {df2.Time.unique()} - {df2.Time.nunique()}\")\n",
    "print(f\"State values {df2.State.unique()} - {df2.State.nunique()}\")\n",
    "print(f\"Group values {df2.Group.unique()} - {df2.Group.nunique()}\")\n",
    "print(\"\\nDescribe df2 after removing spaces\")\n",
    "df2[categorical].describe() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the same to the original df\n",
    "df[\"Time\"] = df[\"Time\"].str.strip()\n",
    "df[\"State\"] = df[\"State\"].str.strip()\n",
    "df[\"Group\"] = df[\"Group\"].str.strip()\n",
    "print(\"\\nDescribe df after removing spaces\")\n",
    "df[categorical].describe() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. In this case I decided to dropna() but you could also fillin with mode() sample because we only have 3 rows (added) with NaN\n",
    "\n",
    "# time_value =list(df2['Time'].mode().sample())[0]\n",
    "# df2.fillna({'Time': time_value }, inplace=True)\n",
    "\n",
    "# group_value =list(df2['Group'].mode().sample())[0]\n",
    "# df2.fillna({'Group': group_value }, inplace=True)\n",
    "\n",
    "# state_value =list(df2['State'].mode().sample())[0]\n",
    "# df2.fillna({'State': group_value }, inplace=True)\n",
    "df2 = df2.dropna\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Choose a suitable Data Wrangling technique——either data standardization or normalization.\n",
    "#### Observations\n",
    "I will use groupby() since we only have one dataset to split my data into categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by categoricals\n",
    "cat_time = df.groupby(\"Time\")\n",
    "cat_group = df.groupby(\"Group\")\n",
    "cat_state = df.groupby(\"State\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by month & year\n",
    "if 'Date'in df.index.names:\n",
    "    df.reset_index(inplace=True)\n",
    "\n",
    "# Make sure the 'Date' column is in datetime format\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df['YearMonth'] = df['Date'].dt.to_period('M')\n",
    "\n",
    "cat_month_year = df.groupby('YearMonth')\n",
    "sales_by_month_year = cat_month_year['Sales'].sum().reset_index()\n",
    "sales_by_month_year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "- Here we can see that we only have three months. \n",
    "- We observe that Dec has more sales compared to Oct and Nov."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data analysis\n",
    "\n",
    "a. Perform descriptive statistical analysis on the data in the Sales and Unit columns. Utilize techniques such as mean, median, mode, and standard deviation for this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Sales.describe():\\n{df.Sales.describe()}\\n\")\n",
    "print(f\"Total Sales: {df['Sales'].sum()}\\n\")\n",
    "print(f\"Total Sales per month: \\n{cat_month_year['Sales'].sum().reset_index()}\\n\")\n",
    "# Based on each category\n",
    "print(f\"By Group:\\n {cat_group.Sales.agg(['mean', 'sum'])}\\n\")\n",
    "print(f\"By Time:\\n {cat_time.Sales.agg(['mean', 'sum'])}\\n\")\n",
    "print(f\"By State:\\n {cat_state.Sales.agg(['mean', 'sum'])}\\n\")\n",
    "\n",
    "# print(f\"Sales median: {df['Sales'].median()}\")\n",
    "# # print(f\"Sales mean: {df['Sales'].mean()}\")\n",
    "# print(f\"Sales mode: {df['Sales'].mode()} \\n\")\n",
    "\n",
    "print(f\"Unit.describe():\\n{df.Unit.describe()}\\n\")\n",
    "print(f\"Total unit: {df['Unit'].sum()}\\n\")\n",
    "print(f\"Total units per month: \\n{cat_month_year['Unit'].sum().reset_index()}\\n\")\n",
    "# Based on each category\n",
    "print(f\"By Group:\\n {cat_group.Unit.agg(['mean', 'sum'])}\\n\")\n",
    "print(f\"By Time:\\n {cat_time.Unit.agg(['mean', 'sum'])}\\n\")\n",
    "print(f\"By State:\\n {cat_state.Unit.agg(['mean', 'sum'])}\\n\")\n",
    "\n",
    "# print(f\"Unit median: {df['Unit'].median()}\")\n",
    "# print(f\"Unit mean: {df['Unit'].mean()}\")\n",
    "# print(f\"Unit mode: {df['Unit'].mode()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Identify the group with the highest sales and the group with the lowest sales based on the data provided.\n",
    "\n",
    "c. Identify the group within the highest and lowest sales based on the data provided.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_group.Sales.sum().nlargest(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "- The group with highest sales is Men.\n",
    "- The group with the lowest sales is Seniours.\n",
    "- The groups within the highest and lowest sales are Women and Kids."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. Generate weekly, monthly, and quarterly reports to document and present the results of the analysis conducted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weekly:           count          mean           std      min      25%      50%  \\\n",
      "YearWeek                                                                 \n",
      "2020-39   252.0  44940.476190  30401.470867   7500.0  21875.0  33750.0   \n",
      "2020-40   588.0  45663.265306  29662.463059   7500.0  22500.0  37500.0   \n",
      "2020-41   588.0  45680.272109  29957.817907   7500.0  22500.0  35000.0   \n",
      "2020-42   588.0  45225.340136  29520.627513   7500.0  22500.0  35000.0   \n",
      "2020-43   504.0  44965.277778  30211.502370   7500.0  20000.0  33750.0   \n",
      "2020-44   588.0  35161.564626  26608.306249   5000.0  15000.0  25000.0   \n",
      "2020-45   588.0  36156.462585  27559.401709   5000.0  15000.0  25000.0   \n",
      "2020-46   588.0  36339.285714  27706.605418   5000.0  15000.0  25000.0   \n",
      "2020-47   588.0  35901.360544  27185.080176   5000.0  15000.0  25000.0   \n",
      "2020-48   588.0  48975.340136  34674.229649   5000.0  22500.0  37500.0   \n",
      "2020-49   588.0  52687.074830  35826.917392  12500.0  25000.0  37500.0   \n",
      "2020-50   588.0  54094.387755  36576.190845  12500.0  25000.0  37500.0   \n",
      "2020-51   588.0  53715.986395  37214.399535  12500.0  25000.0  38750.0   \n",
      "2020-52   336.0  54851.190476  36573.583956  12500.0  25000.0  37500.0   \n",
      "\n",
      "              75%       max  \n",
      "YearWeek                     \n",
      "2020-39   67500.0  125000.0  \n",
      "2020-40   67500.0  125000.0  \n",
      "2020-41   67500.0  125000.0  \n",
      "2020-42   67500.0  125000.0  \n",
      "2020-43   67500.0  125000.0  \n",
      "2020-44   52500.0  112500.0  \n",
      "2020-45   52500.0  112500.0  \n",
      "2020-46   55000.0  112500.0  \n",
      "2020-47   52500.0  112500.0  \n",
      "2020-48   67500.0  162500.0  \n",
      "2020-49   75000.0  162500.0  \n",
      "2020-50   77500.0  162500.0  \n",
      "2020-51   75625.0  162500.0  \n",
      "2020-52   80000.0  162500.0   \n",
      "\n",
      "Monthly:             count          mean           std      min      25%      50%  \\\n",
      "YearMonth                                                                  \n",
      "2020-10    2520.0  45353.174603  29861.302213   7500.0  22500.0  35000.0   \n",
      "2020-11    2520.0  35985.119048  27366.175823   5000.0  15000.0  25000.0   \n",
      "2020-12    2520.0  53702.380952  36385.451298  12500.0  25000.0  37500.0   \n",
      "\n",
      "               75%       max  \n",
      "YearMonth                     \n",
      "2020-10    67500.0  125000.0  \n",
      "2020-11    55000.0  112500.0  \n",
      "2020-12    77500.0  162500.0  \n",
      "\n",
      "Quarterly:               count          mean           std     min      25%      50%  \\\n",
      "YearQuarter                                                                 \n",
      "2020Q4       7560.0  45013.558201  32253.506944  5000.0  20000.0  35000.0   \n",
      "\n",
      "                 75%       max  \n",
      "YearQuarter                     \n",
      "2020Q4       65000.0  162500.0  \n"
     ]
    }
   ],
   "source": [
    "# Create a new columns 'YearQuarter' and 'YearWeek' to hold the year, quarter and week number for each date\n",
    "\n",
    "df['YearQuarter'] = df['Date'].dt.to_period('Q')\n",
    "df['YearWeek'] = df['Date'].dt.strftime('%Y-%U')\n",
    "\n",
    "# Describe each group\n",
    "print(f\"Weekly: {df.groupby('YearWeek').Sales.describe()} \\n\")\n",
    "print(f\"Monthly: {df.groupby('YearMonth').Sales.describe()}\\n\")\n",
    "print(f\"Quarterly: {df.groupby('YearQuarter').Sales.describe()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLAIBootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
